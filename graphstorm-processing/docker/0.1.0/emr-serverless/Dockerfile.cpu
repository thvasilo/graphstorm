FROM public.ecr.aws/emr-serverless/spark/emr-6.11.0:20230629-x86_64 as runtime
USER root
ENV PYTHON_VERSION=3.9.18

# Python wonâ€™t try to write .pyc or .pyo files on the import of source modules
# Force stdin, stdout and stderr to be totally unbuffered. Good for logging
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV PYTHONIOENCODING=UTF-8

# Set up pyenv
ENV PYENV_ROOT="${HOME}/.pyenv"
ENV PATH="${PYENV_ROOT}/shims:${PYENV_ROOT}/bin:${PATH}"
ENV PYSPARK_DRIVER_PYTHON=${PYENV_ROOT}/shims/python
ENV PYSPARK_PYTHON=${PYENV_ROOT}/shims/python

# TODO: These can probably all go to another builder stage?
RUN yum erase -y openssl-devel && \
    yum install -y \
        bzip2-devel\
        gcc \
        git \
        libffi-devel \
        ncurses-devel \
        openssl11-devel \
        readline-devel \
        sqlite-devel \
        sudo \
        xz-devel && \
        rm -rf /var/cache/yum
RUN git clone https://github.com/pyenv/pyenv.git ${PYENV_ROOT} && \
    pyenv install ${PYTHON_VERSION} && \
    pyenv global ${PYTHON_VERSION}
    # source ${VIRTUAL_ENV}/bin/activate


# GSProcessing codebase
COPY code/ /usr/lib/spark/code/
WORKDIR /usr/lib/spark/code/

# ENTRYPOINT [ "docker-entry.sh" ]

# TODO: Create env separately for the lib install and just use that, with poetry
FROM runtime AS prod
RUN python -m pip install /usr/lib/spark/code/graphstorm_processing-*.whl && \
    rm /usr/lib/spark/code/graphstorm_processing-*.whl && rm -rf /root/.cache
# CMD ["gs-processing"]

FROM runtime AS test
RUN python -m pip install /usr/lib/spark/code/graphstorm-processing/ && rm -rf /root/.cache
# CMD ["sh", "-c", "pytest ./code/tests/"]

USER hadoop:hadoop
WORKDIR /home/hadoop
